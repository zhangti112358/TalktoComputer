让计算机听懂人话！

# 使用示例
<video src="doc/demo/demo_25.3.16.mp4" controls width="640" height="360"></video>

# 运行

npm install

npm run dev    # mac

npm run devwin # windows 区别是运行前执行“chcp 65001”改为utf-8编码

# 人机交互可以更加自然且高效

人使用计算机的历史是：

1. 二进制数据
2. 命令行
3. 图形界面

随着计算机对信息处理能力的提升，人机交互的趋势有：

1. 交互方式更自然。
2. 交互的信息种类增加，效率更高。

现在LLM进一步提升了对信息的理解能力，因此可以把交互做得更好。

# 需要探索的方向

如果用图形界面类比，当前实现的功能大概相当于“屏幕上有了图标，可以点击”。像“双指缩放”，“惯性滑动”这样好用的操作还没有，也还没有好用的app。

至于怎么做，这些是值得关注的：

硬件交互。比如带麦克风和相机的眼镜

1. 更便捷的语音交互
2. 实时获取人眼看到的信息后能做什么

软件交互，理解需求和执行。比如各种chatbot，computer use，agent工具

1. 获取完整上下文 使用场景，过去/现在的消息，屏幕等用户感知到的信息
2. 需求理解
   1. 各种模型能力的了解。对话/语音/图像视频/特征提取等
   2. 怎么把想要的输入输出和模型结合，达到较高的可用性。
3. 调用软件
   1. 基于api调用。比如MCP，定义接口含义和各参数给LLM判断
   2. 基于GUI调用。比如AutoGLM，使用录屏和模拟点击的方式

# 讨论与合作

邮箱：zhangti112358@qq.com

微信：zhangti112358 （备注“语音”加入讨论群）
